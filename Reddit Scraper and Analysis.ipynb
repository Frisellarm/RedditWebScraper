{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set up html request function for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html request\n",
    "def html_data(url):\n",
    "    html = requests.get(url, headers = {'User-Agent': 'whatever'})\n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "    threads = soup.find_all(name = 'div', attrs = {'id':'siteTable','class':'sitetable linklisting'})\n",
    "    \n",
    "    return threads, soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions for scraping reddit front page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(threads, titles):\n",
    "    for i in threads:\n",
    "        x = i.find_all(name = 'p', attrs = {'class':'title'})\n",
    "        for t in x:\n",
    "               titles.append(t.a.text) \n",
    "    return titles\n",
    "            \n",
    "def get_subreddits(threads, subreddit):\n",
    "    for i in threads:\n",
    "        x = i.find_all(name = 'p', attrs = {'class':'tagline'})\n",
    "        for t in x:\n",
    "            subreddit.append(t.find('a', attrs = {'class':'subreddit hover may-blank'}).text)\n",
    "    return subreddit\n",
    "\n",
    "def get_dates(threads, dates):\n",
    "    for i in threads:\n",
    "        x = i.find_all(name = 'p', attrs = {'class':'tagline'})\n",
    "        for d in x:\n",
    "            dates.append(d.find(name = 'time')['datetime'])\n",
    "    return dates\n",
    "\n",
    "def get_vids(threads, vids):\n",
    "    for i in threads:\n",
    "        x = i.find_all(name = 'div', attrs = {'class':'top-matter'})\n",
    "        for d in x:\n",
    "            if d.find(name = 'div', attrs = {'class':'expando-button collapsed hide-when-pinned video'}) != None:\n",
    "                vids.append(1)\n",
    "            else:\n",
    "                vids.append(0)\n",
    "    return vids\n",
    "    \n",
    "def get_thread_url(threads, threadlink):\n",
    "    for i in threads:\n",
    "        x = i.find_all(name = 'p', attrs = {'class':'title'})\n",
    "        for t in x:\n",
    "               threadlink.append(t.a['href'])\n",
    "    return threadlink\n",
    "    \n",
    "def get_comments(threads, comments):\n",
    "    for i in threads:\n",
    "        x = i.find_all(name = 'li', attrs = {'class':'first'})\n",
    "        for j in x:\n",
    "            comments.append(j.find(name = 'a').text.split()[0])\n",
    "    return comments\n",
    "\n",
    "#if no likes return NaN, process this with Pandas later(likely remove)\n",
    "def get_likes(threads, likes):\n",
    "    for i in threads:\n",
    "        x = i.find_all(name = 'div', attrs = {'class':'score unvoted'})\n",
    "        for j in x:\n",
    "            try:\n",
    "                likes.append(j['title'])\n",
    "            except:\n",
    "                likes.append('NaN')\n",
    "    return likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape and consolidate data into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape the data\n",
    "def getFrontPageData(pages = 5, url = 'https://www.reddit.com/'):\n",
    "    '''Web scrapes reddit for front page data, you can input how many pages you want to return.\n",
    "       If this number exceeds the total number of pages available the function will return all results.\n",
    "       \n",
    "       If the scraper crashes when attempting to reach the next page it will output the current results\n",
    "       of the dataframe. If you need to pick up at a later time you can input the url you want to start \n",
    "       at by using the url kwarg.'''\n",
    "    \n",
    "    #create lists/dict for information we gonna scrape\n",
    "    titles, subreddit, dates, threadlink, vids, comments, likes = ([] for i in range(7))\n",
    "    red_dict = {}\n",
    "    \n",
    "    #keeps track of results per page\n",
    "    page_cnt = 0\n",
    "   \n",
    "    for page in range(0, pages):\n",
    "        #catch soup and data about trending threads\n",
    "        threads, soup = html_data(url)\n",
    "\n",
    "        #try to get data, end function if error\n",
    "        try:\n",
    "            titles = get_titles(threads, titles)\n",
    "            subreddit = get_subreddits(threads, subreddit)\n",
    "            dates = get_dates(threads, dates)\n",
    "            threadlink = get_thread_url(threads, threadlink)\n",
    "            vids = get_vids(threads, vids)\n",
    "            comments = get_comments(threads, comments)\n",
    "            likes = get_likes(threads, likes)\n",
    "        except:\n",
    "            print(\"Getting Data Error!\")\n",
    "            break\n",
    "\n",
    "        #get hyperlink for next page, if not available this is the end of the front page, end script\n",
    "        #by breaking out of for loop and creating the dataframe, increase page count by 25\n",
    "        try:\n",
    "            page_cnt += 25\n",
    "            for i in threads:\n",
    "                x = i.find(name = 'div')['class'][2].split('-')[1]\n",
    "            url = 'https://www.reddit.com/?count=' + str(page_cnt) + '&after=' + str(x)\n",
    "        except:\n",
    "            print(\"Next Page Error!\")\n",
    "            break\n",
    "            \n",
    "        #wait 1 sec, get next page data\n",
    "        time.sleep(1)\n",
    "        \n",
    "        #print number of results in iterations of 1000\n",
    "        if (page % 1000) == 0:\n",
    "            print(\"Current Results\", page)\n",
    "    \n",
    "    #create dictionary from lists for creating df\n",
    "    red_dict['title'] = titles\n",
    "    red_dict['subreddit'] = subreddit\n",
    "    red_dict['submitted'] = dates\n",
    "    red_dict['threadlink'] = threadlink\n",
    "    red_dict['comments'] = comments\n",
    "    red_dict['expands'] = vids\n",
    "    red_dict['likes'] = likes\n",
    "\n",
    "    #printing information where scrape ends\n",
    "    print(\"Scraped results for\", page + 1, \"reddit page(s)\")\n",
    "    print(\"Returned\", len(titles), \"thread results\")\n",
    "    print(\"Last scraped Reddit page:\", url)\n",
    "    \n",
    "    #return df to begin analysis\n",
    "    return pd.DataFrame(red_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Reddit and return results as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#input of 10,000 pages should return 250,000 results if no errors\n",
    "red = getFrontPageData(pages= 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv in current directory so this does not have to be rerun\n",
    "red.to_csv('reddit_threads.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
